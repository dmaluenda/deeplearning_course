{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><sub>This notebook is distributed under the <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\" target=\"_blank\">Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license</a>.</sub></div>\n",
    "<h1>Hands on Machine Learning  <span style=\"font-size:12px;\"><i>by <a href=\"https://webgrec.ub.edu/webpages/000004/cat/dmaluenda.ub.edu.html\" target=\"_blank\">David Maluenda</a></i></span></h1>\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://atenea.upc.edu/course/view.php?id=85709\" target=\"_blank\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/upc_logo_49px.png\" width=\"130\"/>\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "  </td>\n",
    "  <td>   <!-- gColab -->\n",
    "    <a href=\"https://colab.research.google.com/github/dmaluenda/hands_on_machine_learning/blob/master/03_CNN_Autoencoder.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/colab_logo_32px.png\" />\n",
    "      Run in Google Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- github -->\n",
    "    <a href=\"https://github.com/dmaluenda/hands_on_machine_learning/blob/master/03_CNN_Autoencoder.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/github_logo_32px.png\" />\n",
    "      View source on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- download -->\n",
    "    <a href=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/03_CNN_Autoencoder.ipynb\"  target=\"_blank\"\n",
    "          download=\"03_CNN_Autoencoder\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/download_logo_32px.png\" />\n",
    "      Download notebook\n",
    "      </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\text{III}$. Convolutional Neural Networks and Autoencoders (using Keras)\n",
    "\n",
    "Hands on \"Machine Learning on Classical and Quantum data\" course of\n",
    "[Master in Photonics - PHOTONICS BCN](https://photonics.masters.upc.edu/en/general-information)\n",
    "[[UPC](https://photonics.masters.upc.edu/en) +\n",
    "[UB](https://www.ub.edu/web/ub/en/estudis/oferta_formativa/master_universitari/fitxa/P/M0D0H/index.html?) +\n",
    "[UAB](https://www.uab.cat/en/uab-official-masters-degrees-study-guides/) +\n",
    "[ICFO](https://www.icfo.eu/lang/studies/master-studies)].\n",
    "\n",
    "Tutorial 3\n",
    "\n",
    "This notebook shows how to:\n",
    "- implement a neural network using the Tensorflow/Keras module\n",
    "- recognize/classify images with dense nets (supervised learning)\n",
    "- recognize/classify images with convolutional nets (supervised learning)\n",
    "- implement image denoising using pseudo-autoencoders (almost unsupervised learning)\n",
    "- dimension reduction with autoencoders\n",
    "- quick review of: U-net, GAN, cGAN and VAE (unsupervised learning)\n",
    "- understand train, validation and test datasets\n",
    "- implement callbacks, like an automatic early stopper\n",
    "\n",
    "**References**:\n",
    "\n",
    "[1] [Machine Learning for Physicists](https://machine-learning-for-physicists.org/) by Florian Marquardt.<br>\n",
    "[2] [Keras](https://keras.io/getting_started/): a deep learning API written in Python.<br>\n",
    "[3] [Tensorflow](https://www.tensorflow.org/api_docs/python/tf): an open source machine learning platform.<br>\n",
    "[4] [Using neural nets to recognize handwritten digits](http://neuralnetworksanddeeplearning.com/chap1.html).<br>\n",
    "[5] [pix2pix](https://www.tensorflow.org/tutorials/generative/pix2pix): Image-to-image translation with a conditional GAN.<br>\n",
    "[6] Dimension reduction in [Towards data science](https://ekamperi.github.io/machine%20learning/2021/01/21/encoder-decoder-model.html) <br>\n",
    "[7] VAE example on [Towards data science](https://towardsdatascience.com/variational-autoencoders-as-generative-models-with-keras-e0c79415a7eb).<br>\n",
    "[8] https://github.com/kartikgill/Autoencoders.<br>\n",
    "[9] https://github.com/dhanushkamath/VariationalAutoencoder. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#0.-Imports:-Basics-and-tensorflow\" data-toc-modified-id=\"0.-Imports:-Basics-and-tensorflow-0\">0. Imports: Basics and tensorflow</a></span></li><li><span><a href=\"#1.-Import-data\" data-toc-modified-id=\"1.-Import-data-1\">1. Import data</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-Labels-preprocessing-(One-hot-encoding-for-classification)\" data-toc-modified-id=\"1.1-Labels-preprocessing-(One-hot-encoding-for-classification)-1.1\">1.1 Labels preprocessing (One hot encoding for classification)</a></span></li></ul></li><li><span><a href=\"#2.-Introduction-to-Tensorflow-and-Keras:-Image-recognition-with-a-fully-connected-neural-network\" data-toc-modified-id=\"2.-Introduction-to-Tensorflow-and-Keras:-Image-recognition-with-a-fully-connected-neural-network-2\">2. Introduction to Tensorflow and Keras: Image recognition with a fully connected neural network</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1-Image-preprocessing-(flatten-images-for-dense-networks)\" data-toc-modified-id=\"2.1-Image-preprocessing-(flatten-images-for-dense-networks)-2.1\">2.1 Image preprocessing (flatten images for dense networks)</a></span></li><li><span><a href=\"#2.2-Network-definition-using-tensorflow/keras\" data-toc-modified-id=\"2.2-Network-definition-using-tensorflow/keras-2.2\">2.2 Network definition using tensorflow/keras</a></span></li><li><span><a href=\"#2.3-One-epoch-training-with-single-steps\" data-toc-modified-id=\"2.3-One-epoch-training-with-single-steps-2.3\">2.3 One epoch training with single steps</a></span></li><li><span><a href=\"#2.4-Train-for-several-epochs\" data-toc-modified-id=\"2.4-Train-for-several-epochs-2.4\">2.4 Train for several epochs</a></span></li></ul></li><li><span><a href=\"#3.-Image-recognition-with-a-CNN\" data-toc-modified-id=\"3.-Image-recognition-with-a-CNN-3\">3. Image recognition with a CNN</a></span></li><li><span><a href=\"#4.-Image-Denoiser-(pseudo-unsupervised-learning)\" data-toc-modified-id=\"4.-Image-Denoiser-(pseudo-unsupervised-learning)-4\">4. Image Denoiser (pseudo-unsupervised learning)</a></span></li><li><span><a href=\"#5.-Autoencoder-for-dimension-reduction-(unsupervised-training)\" data-toc-modified-id=\"5.-Autoencoder-for-dimension-reduction-(unsupervised-training)-5\">5. Autoencoder for dimension reduction (unsupervised training)</a></span></li><li><span><a href=\"#6.-[Just-read]-Autoencoders-for-random-face-generator\" data-toc-modified-id=\"6.-[Just-read]-Autoencoders-for-random-face-generator-6\">6. [Just read] Autoencoders for random face generator</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports: Basics and tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module tensorflow have to be installed.\n",
    "\n",
    "If a 'typeDict' error is triggered, install numpy==1.21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!pip install numpy==1.21`  # it should not be needed. Use it if \"'numpy' has no attribute 'typeDict'\" error\n",
    "\n",
    "`!pip install -U tensorflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import mnist  # dataset of handwritten numbers\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt  # for plotting\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=300  # highres display\n",
    "\n",
    "# for updating display \n",
    "# (very simple animation)\n",
    "from IPython.display import clear_output\n",
    "from time import time, sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import data\n",
    "\n",
    "In this tutorial we will use the MNIST dataset. It is a dataset with many images depicting handwritten numbers and their corresponding numeric value.\n",
    "\n",
    "We will do three exercises with this dataset:\n",
    "\n",
    "1. detect the handwritten numbers (classification)\n",
    "1. denoise images (pseudo-autoencoder)\n",
    "1. dimensional reduction (autoencoder)\n",
    "\n",
    "One key point on using data for training neural networks is to have three independent subsets.\n",
    "\n",
    " - Training dataset: It is used for training. The biggest subset, typically around 70%.\n",
    " - Validation dataset: It is used to monitor the training, but not used for backpropagation. Typically around 15%.\n",
    " - Test dataset: It is used only after the training in order to check the performance of the neural network. Typically around 15%.\n",
    " \n",
    "The function below returns a whole dataset of 70k handwritten numbers and their corresponding number. It is split in this three independent sub-datasets with 50k for training, 10k for validation, and 10k for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mnist_loader\n",
    "~~~~~~~~~~~~\n",
    "\n",
    "taken from Nielsen's online book:\n",
    "http://neuralnetworksanddeeplearning.com/chap1.html\n",
    "\n",
    "\n",
    "A library to load the MNIST image data.  For details of the data\n",
    "structures that are returned, see the doc strings for ``load_data``\n",
    "and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the\n",
    "function usually called by our neural network code.\n",
    "\"\"\"\n",
    "\n",
    "def load_data():\n",
    "    \"\"\" Return three datasets (train/val/test), each one being a tuple of\n",
    "         - numpy array of setSize x 28 x 28 x 1: Set of images\n",
    "         - numpy array of setSize x 1: Set of ground truth values\n",
    "        setSize is 50000 for the train set and 10000 for the val and test sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get raw data: a tuple of two entries (train/test) each one \n",
    "    #  being also a tuple of two entries, one for a set of 28x28 images\n",
    "    #  and the other being a set of their corresponding integers\n",
    "    (train_val_X, train_val_y), (test_X, test_y) = mnist.load_data()\n",
    "\n",
    "    # to convert values from 0 to 255 into range 0. to 1.\n",
    "    train_val_X = train_val_X.astype('float32') / 255.\n",
    "    test_X = test_X.astype('float32') / 255.\n",
    "    \n",
    "    # Conv2D need 3D images (row, columns, chanels), so adding the chanel dimension\n",
    "    train_val_X = np.reshape(train_val_X, (len(train_val_X), 28, 28, 1)) \n",
    "    test_X = np.reshape(test_X, (len(test_X), 28, 28, 1))\n",
    "    \n",
    "    train_X = train_val_X[:-10000]\n",
    "    val_X = train_val_X[-10000:]\n",
    "\n",
    "    train_y = train_val_y[:-10000]\n",
    "    val_y = train_val_y[-10000:]\n",
    "    \n",
    "    return (train_X, train_y), (val_X, val_y), (test_X, test_y)\n",
    "\n",
    "(train_x, train_y), (val_x, val_y), (test_x, test_y) = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to check a handwritten number. run it several times to see check different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train_x:\", train_x.shape)\n",
    "print(\"train_y:\", train_y.shape)\n",
    "\n",
    "print(\"val_x:\", val_x.shape)\n",
    "print(\"val_y:\", val_y.shape)\n",
    "\n",
    "print(\"test_x:\", test_x.shape)\n",
    "print(\"test_y:\", test_y.shape)\n",
    "\n",
    "idx = np.random.randint(0,50000)\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(train_x[idx,:,:], cmap='gray')\n",
    "plt.title(train_y[idx]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Labels preprocessing (One hot encoding for classification)\n",
    "\n",
    "The best strategy for categorical classification (classify inputs into categories) is to have one dedicated output neuron for each possible category. In this way, the assigned category for a given input will correspond to the most activated (hottest) output neuron. This is called one hot encoding.\n",
    "\n",
    "Check [this](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) for more information on one hot encoding.\n",
    "\n",
    "Therefore, the first step is to prepare the dataset in this way.\n",
    "\n",
    "The MNIST dataset contains pairs of $x$ images and $y$ labels. The labels are integers corresponding to the truly handwritten value. Then, take the three imported raw datasets and create $y$ vectorized labels as _one hot encoding_, so they should be vectors of 10 components where all them are zero, except for the corresponding true handwritten number.\n",
    "\n",
    "For example, a $3$ should be `[0 0 0 1 0 0 0 0 0 0 0]` and a $7\\rightarrow$ `[0 0 0 0 0 0 0 1 0 0]` and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Tensorflow and Keras: Image recognition with a fully connected neural network\n",
    "\n",
    "Let's start with a fully connected network (also known as dense networks or perceptron).\n",
    "They are not convolutional networks, but let's start with the simplest case, just to introduce tensorflow and keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Image preprocessing (flatten images for dense networks)\n",
    "\n",
    "The input of fully connected neural networks (dense networks) are several neurons, corresponding to a vector (1D). However, we have images as input data (2D). So, we have to flatten that images. So, take the images from the three raw datasets and flatten them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to show a handwritten number where the title contains the one hot encoding vector. Take the flatten image and reshape it to check all works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Network definition using tensorflow/keras\n",
    "\n",
    "Let's define a dense network with one hidden layer containing about 30 neurons.\n",
    "\n",
    "Check [`tensorflow.keras.Sequencial`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) class and its methods, e.g. [`add()`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#add) to create a neural network model.\n",
    "\n",
    "Check also [`tensorflow.keras.layers.Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) to add fully connected layers.\n",
    "\n",
    "For categorical classification, the output layer should deal with a [softmax activation function](https://gombru.github.io/2018/05/23/cross_entropy_loss/). The hidden layer can be done with any other activation function (relu, sigmoid, etc) try different activation function for the hidden layer and check its behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the method [`compile()`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#compile) to configure the model for training.\n",
    "\n",
    "This means to assign a loss function, an optimizer, and some metrics.\n",
    "\n",
    " - The loss function is to compute the backpropagation. For the one hot encoding classification, the most sensitive loss function is the categorical cross entropy.\n",
    " - The most used optimizer is the Adam, an adaptive learning rate based on the momentum statistics. Check [`tensorflow.keras.optimizers.adam`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam).\n",
    " - The metrics are evaluated also over the validation dataset and they are stored in the history to show the progress of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `summary()` method of your sequential network to check the created model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 One epoch training with single steps\n",
    "\n",
    "Train one epoch (the whole training dataset) on batches of 100 items by using the [`train_on_batch()`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#train_on_batch) method of your model.\n",
    "\n",
    "Remember that one epoch is considered when the whole training dataset is used, whereas one step is one backpropagation made over one batch of data.\n",
    "So, `steps * batchsize = training_dataset_size`.\n",
    "\n",
    "The `train_on_batch()` returns the cost of that certain training step. Store all that costs over all the iterations and plot them at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the network performance by doing some predictions on the test dataset and comparing them to the ground truth.\n",
    "\n",
    "You can use the method [`predict_on_batch()`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#predict_on_batch) to check the behavior of the training after one epoch on the test dataset. \n",
    "\n",
    "Remember that the output is _one hot encoded_, so the predicted vector is a set of probabilities. The predicted value is the maximum one.\n",
    "\n",
    "Then compare the predicted handwritten value with the ground truth and compute the accuracy as the number of good predictions over the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some wrong predicted images and print the probabilities obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train for several epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To refine the training, it can be done several epochs, let's say 30. Check the [`fit()`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#fit) method, where you can set the batchsize and the number of epochs to train. This method, returns a history. Store that history to check it after.\n",
    "\n",
    "Also, measure the elapsed time in order to compare it with the convolutional network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the stored history. Which type of variable it is? Check also `history.history`.\n",
    "\n",
    "`history.history` contains the losses and the metrics, for both the training and the validation datasets. Plot some of that curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happen if validation loss/accuracy is saturated while the training loss/accuracy is getting better? Is that situation good?\n",
    "\n",
    "Check again the accuracy of the model over the test dataset. Has it increase after training with more epochs? and training it with more and more epochs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image recognition with a CNN\n",
    "\n",
    "Let's do the same with a Convolutional Neural Network. Just to recall what they are:\n",
    "\n",
    "![https://miro.medium.com/max/1400/1*vkQ0hXDaQv57sALXAJquxA.jpeg](https://miro.medium.com/max/1400/1*vkQ0hXDaQv57sALXAJquxA.jpeg)\n",
    "![https://docs.ecognition.com/Resources/Images/ECogUsr/UG_CNN_scheme.png](https://docs.ecognition.com/Resources/Images/ECogUsr/UG_CNN_scheme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a convolutional neural network of three bottle necks (cnn2d+maxPooling). Use again a `Sequential()` model and use `tf.keras.layers.Conv2D` and `tf.keras.layers.MaxPooling2D` layers.\n",
    "\n",
    "Convolutional layers deals with 2D data. However, we want a 1D output of 10 neurons (one hot encoding vector). So use the `tf.keras.layers.Flatten` layer and add a final `Dense` layer. Remember, this last layer should have the softmax activation function, while the convolutional could be (relu, sigmoid...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the summary of the model. Check how many *trainable parameters* it have and compare it with the fully connected network done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `fit` method to train the model with 10 epochs. Check the elapsed time and compare it with the fully connected. Also store the history and plot the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the accuracy of predictions over the test dataset and compare it with the dense model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Denoiser (pseudo-unsupervised learning)\n",
    "\n",
    "Let's try to remove noise from noisy images.\n",
    "\n",
    "An autoencoder is a model made of two parts: encoder + decoder, having a bottle neck in the middle (a flatten vector typically called code or embedding).\n",
    "\n",
    "Usually, autoencoders are fed with the same image $x$ in both, the input and the output. However, we can slightly modify this by setting a $x$ noisy image in the input and a $x'$ clean image in the output. Then, the model should learn to remove noise of noisy images.\n",
    "\n",
    "![https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/Autoencoder_structure.png/350px-Autoencoder_structure.png](https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/Autoencoder_structure.png/350px-Autoencoder_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to add noise to the images. Then we can use that as input, while the original will be the clean output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ae, vl_ae, te_ae = load_data()\n",
    "\n",
    "train_clean = tr_ae[0]\n",
    "val_clean = vl_ae[0]\n",
    "test_clean = te_ae[0]\n",
    "\n",
    "noise_factor = 0.5\n",
    "# random values as noise source\n",
    "train_noisy = train_clean + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=train_clean.shape)  \n",
    "val_noisy = val_clean + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=val_clean.shape) \n",
    "test_noisy = test_clean + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=test_clean.shape) \n",
    "\n",
    "# to make values in the range of 0 to 1: values<0 -> 0 while values>1 -> 1.\n",
    "train_noisy = np.clip(train_noisy, 0., 1.)   \n",
    "val_noisy = np.clip(val_noisy, 0., 1.)\n",
    "test_noisy = np.clip(test_noisy, 0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_clean[i,:,:], cmap='gray')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_noisy[i,:,:], cmap='gray')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the autoencoder having three downsampling steps in the encoder and three upsamplig steps in the decoder.\n",
    "\n",
    "In this case, try to do it in the _functional_ approach, instead of adding layers with the `add()` method. It is the same, but sometimes it is more convenient.\n",
    "\n",
    "Check https://www.tensorflow.org/guide/keras/functional_api#introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model some epochs using the `fit()` method. In this case, let's learn some features provided by tensorflow.\n",
    "\n",
    "Check the `shuffle` input parameter of the `fit` function and activate it. What it does? Why can be useful?\n",
    "\n",
    "Another possibility is to add callbacks on the `fit` method. The callbacks are functions or classes that are call after every training step. In this case, try to add an [`EarlyStopping()`](https://keras.io/api/callbacks/early_stopping/) callback to finish the training when the loss is saturated for **three epochs** on the **validation dataset** with a **minimum threshold of $0.005$**, set also the `verbose=1` to monitor it behavior. To be sure that the early stopper is triggered, set a huge number of epochs.\n",
    "\n",
    "Plot the progress via the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the noise on the test dataset using the `predict()` method.\n",
    "\n",
    "Make a subplot having three rows and 5 columns to show 5 different images. Show in the first row the clean image, the noisy in the second and, finally, the noise-removed image by the model in the third.\n",
    "\n",
    "If the result is not quite satisfactory, try to train again the model or to modify the parameters on the conv2D layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Check better results on [the original page](https://keras.io/examples/vision/autoencoder/)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Autoencoder for dimension reduction (unsupervised training)\n",
    "\n",
    "Let's do a truly autoencoder where the input and the output is forced to be the same. It can seem stupid, but we will force to pass through a bottle neck, the embedding. Then, the model will learn to condensate all the information on a image to a few of numbers.\n",
    "\n",
    "Let's set an embedding of just two neurons.\n",
    "\n",
    "So, make an autoencoder, made of two separated parts, the encoder and the decoder. So, this is two separated `Sequential` models and then, merged by something like `tf.keras.Model(inputs=encoder.input, outputs=decoder(encoder.output))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit it with the images for 10 epochs and plot the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate fake digits by setting random numbers in the embedding vector, and run (predict) the decoder model with that random numbers. What you see? It is a number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a grid of $9\\times9$ subplots showing the fake digit generated by increasing/decreasing the value of one or the other component of the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. [Just read] Autoencoders for random face generator\n",
    "\n",
    "Check the [`random_face_generator.ipynb` notebook](https://github.com/dhanushkamath/VariationalAutoencoder/blob/master/Variational_Autoencoder.ipynb), it is based on the same principle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "329.125px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
