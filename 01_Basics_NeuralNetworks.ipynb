{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><sub>This notebook is distributed under the <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\" target=\"_blank\">Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license</a>.</sub></div>\n",
    "<h1>Hands on Machine Learning  <span style=\"font-size:10px;\"><i>by <a href=\"https://webgrec.ub.edu/webpages/000004/ang/dmaluenda.ub.edu.html\" target=\"_blank\">David Maluenda</a></i></span></h1>\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://atenea.upc.edu/course/view.php?id=71605\" target=\"_blank\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/upc_logo_49px.png\" width=\"130\"/>\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "  </td>\n",
    "  <td>   <!-- gColab -->\n",
    "    <a href=\"https://colab.research.google.com/github/dmaluenda/hands_on_machine_learning/blob/master/01_Basics_NeuralNetworks.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/colab_logo_32px.png\" />\n",
    "      Run in Google Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- github -->\n",
    "    <a href=\"https://github.com/dmaluenda/hands_on_machine_learning/blob/master/01_Basics_NeuralNetworks.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/github_logo_32px.png\" />\n",
    "      View source on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- download -->\n",
    "    <a href=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/01_Basics_NeuralNetworks.ipynb\"  target=\"_blank\" download=\"01_Basics_NeuralNetworks\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/download_logo_32px.png\" />\n",
    "      Download notebook\n",
    "      </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\text{I}$. Neural Networks with Pure Python\n",
    "\n",
    "Hands on \"Machine Learning on Classical and Quantum data\" course of\n",
    "[Master in Photonics - PHOTONICS BCN](https://photonics.masters.upc.edu/en/general-information)\n",
    "[[UPC](https://photonics.masters.upc.edu/en) +\n",
    "[UB](https://www.ub.edu/web/ub/en/estudis/oferta_formativa/master_universitari/fitxa/P/M0D0H/index.html?) +\n",
    "[UAB](https://www.uab.cat/web/estudiar/la-oferta-de-masteres-oficiales/informacion-general-1096480309770.html?param1=1096482863713) +\n",
    "[ICFO](https://www.icfo.eu/lang/studies/master-studies)].\n",
    "\n",
    "Tutorial 1\n",
    "\n",
    "This notebook shows how to:\n",
    "- implement the forward-pass (evaluation) of a deep, fully connected neural network in a few lines of python\n",
    "- do that efficiently using batches\n",
    "- illustrate the results for randomly initialized neural networks\n",
    "- understand the role of weights and biases in networks\n",
    "- understand the activation functions meanings and usages\n",
    "\n",
    "**References**:\n",
    "\n",
    "[1] [Machine Learning for Physicists](https://machine-learning-for-physicists.org/) by Florian Marquardt.<br>\n",
    "[2] [NumPy](https://numpy.org/doc/stable/user/whatisnumpy.html): the fundamental package for scientific computing in Python.<br>\n",
    "[3] [Matplotlib](https://matplotlib.org/stable/tutorials/introductory/usage.html): a comprehensive library for creating static, animated, and interactive visualizations in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#0-Imports:-only-numpy-and-matplotlib\" data-toc-modified-id=\"0-Imports:-only-numpy-and-matplotlib-0\">0 Imports: only numpy and matplotlib</a></span></li><li><span><a href=\"#1-A-very-simple-neural-network-(no-hidden-layer)\" data-toc-modified-id=\"1-A-very-simple-neural-network-(no-hidden-layer)-1\">1 A very simple neural network (no hidden layer)</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-Simple-implementation-(single-input)\" data-toc-modified-id=\"1.1-Simple-implementation-(single-input)-1.1\">1.1 Simple implementation (single input)</a></span></li><li><span><a href=\"#1.2-More-compact\" data-toc-modified-id=\"1.2-More-compact-1.2\">1.2 More compact</a></span></li><li><span><a href=\"#1.3-Multiple-inputs\" data-toc-modified-id=\"1.3-Multiple-inputs-1.3\">1.3 Multiple inputs</a></span></li><li><span><a href=\"#1.4-[EXERCISE]:-Try-to-get-an-output-image-using-another-activation-functions\" data-toc-modified-id=\"1.4-[EXERCISE]:-Try-to-get-an-output-image-using-another-activation-functions-1.4\">1.4 [EXERCISE]: Try to get an output image using another activation functions</a></span></li></ul></li><li><span><a href=\"#2-NN-with-one-hidden-layer\" data-toc-modified-id=\"2-NN-with-one-hidden-layer-2\">2 NN with one hidden layer</a></span></li><li><span><a href=\"#3-'batch'-processing-of-Neural-Networks\" data-toc-modified-id=\"3-'batch'-processing-of-Neural-Networks-3\">3 'batch' processing of Neural Networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1-Matrix-vector-multiplication-of-array-dimensions\" data-toc-modified-id=\"3.1-Matrix-vector-multiplication-of-array-dimensions-3.1\">3.1 Matrix-vector multiplication of array dimensions</a></span></li><li><span><a href=\"#3.2-Defining-functions-to-evaluate-a-layer,-and-a-network-with-batch-processing\" data-toc-modified-id=\"3.2-Defining-functions-to-evaluate-a-layer,-and-a-network-with-batch-processing-3.2\">3.2 Defining functions to evaluate a layer, and a network with batch processing</a></span></li><li><span><a href=\"#3.3-Now-visualize-this-multi-layer-net,-now-more-efficiently!\" data-toc-modified-id=\"3.3-Now-visualize-this-multi-layer-net,-now-more-efficiently!-3.3\">3.3 Now visualize this multi-layer net, now more efficiently!</a></span></li><li><span><a href=\"#3.4-A-network-with-MANY-hidden-layers-(same-size-each)\" data-toc-modified-id=\"3.4-A-network-with-MANY-hidden-layers-(same-size-each)-3.4\">3.4 A network with MANY hidden layers (same size each)</a></span></li></ul></li><li><span><a href=\"#4-Summing-up-in-general-functions\" data-toc-modified-id=\"4-Summing-up-in-general-functions-4\">4 Summing up in general functions</a></span></li><li><span><a href=\"#5-Fancy-visualization-of-Neural-Networks-with-Pure-Python\" data-toc-modified-id=\"5-Fancy-visualization-of-Neural-Networks-with-Pure-Python-5\">5 Fancy visualization of Neural Networks with Pure Python</a></span><ul class=\"toc-item\"><li><span><a href=\"#5.1-Some-internal-routines-for-plotting-the-network\" data-toc-modified-id=\"5.1-Some-internal-routines-for-plotting-the-network-5.1\">5.1 Some internal routines for plotting the network</a></span></li><li><span><a href=\"#5.2-No-hidden-layer-NN\" data-toc-modified-id=\"5.2-No-hidden-layer-NN-5.2\">5.2 No hidden layer NN</a></span></li><li><span><a href=\"#5.3-Deep-dense-NN\" data-toc-modified-id=\"5.3-Deep-dense-NN-5.3\">5.3 Deep dense NN</a></span></li><li><span><a href=\"#5.4-Something-not-random\" data-toc-modified-id=\"5.4-Something-not-random-5.4\">5.4 Something not random</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlT_ZrS-R1rG"
   },
   "source": [
    "## 0 Imports: only numpy and matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T15:54:31.308199Z",
     "iopub.status.busy": "2021-04-19T15:54:31.307840Z",
     "iopub.status.idle": "2021-04-19T15:54:31.896606Z",
     "shell.execute_reply": "2021-04-19T15:54:31.895654Z",
     "shell.execute_reply.started": "2021-04-19T15:54:31.308135Z"
    },
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1626874792493,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "zRmzXX4wR1rG",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the \"numpy\" library for linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# get \"matplotlib\" for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi'] = 300  # highres display\n",
    "from matplotlib.axes._axes import _log as mpl_ax_logger\n",
    "mpl_ax_logger.setLevel('ERROR')  # ignore warnings\n",
    "\n",
    "# for nice inset colorbars:\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "# time control to count it and manage it\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T98H_rSNR1rH"
   },
   "source": [
    "## 1 A very simple neural network (no hidden layer)\n",
    "\n",
    "The behavior of a neural network with $N_0$ input neurons and $N_1$ output neurons (no hidden layer) is\n",
    "\n",
    "\\begin{equation}\n",
    "y^{\\rm out}_j = f(\\sum_k^{N_0} w_{jk} y^{\\rm in}_k + b_j) \\quad ; \\quad j=1\\dots N_1\n",
    "\\label{eq:simpleNN}\n",
    "\\end{equation}\n",
    "\n",
    "where $y^{\\rm in}_k$ is the input of the $k$-th input neuron,\n",
    "$y^{\\rm out}_j$ is the output of the $j$-th output neuron,\n",
    "$w_{jk}$ is the weight of the connection between the $k$-th input neuron and the $j$-th output neuron,\n",
    "$b_j$ is the bias of the $j$-th output neuron,\n",
    "and $f(Â·)$ is the activation function (usually it is a non-linear: for instance a sigmoid function).\n",
    "\n",
    "Notice that we can define a matrix $w$ of size $N_1\\times N_0$,\n",
    "which contains all $w_{jk}$ connection weights.\n",
    "In the same way, we can condensate all biases $b_j$ in the $b$ vector of size $N_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Simple implementation (single input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1626874814911,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "xtRaehMZR1rI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N0 = 3  # input layer size\n",
    "N1 = 2  # output layer size\n",
    "\n",
    "# initialize random weights. Array dimensions: N1 x N0\n",
    "w = np.random.uniform(low=-1, high=+1, size=(N1,N0))\n",
    "\n",
    "# initialize random biases. Vector of N1 elements\n",
    "b = np.random.uniform(low=-1, high=+1, size=N1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with an arbitrary input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1626874822256,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "LY1EGJKoR1rI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# input values\n",
    "y_in = np.array([0.2, 0.4, -0.1])  # Why three values here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 225,
     "status": "ok",
     "timestamp": 1626874831072,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "SR-4V6z2R1rJ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# evaluating network, Eq. (1). In two steps\n",
    "z = np.dot(w, y_in) + b  # result: vector of 'z' values, length N1\n",
    "y_out = 1 / (1 + np.exp(-z))  # the 'sigmoid' activation function (applied elementwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1626874836630,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "KTpiYEIFR1rJ",
    "outputId": "d13d700c-1365-4dbf-d2e1-29aa014625ee",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"network input y_in:\", y_in)\n",
    "print(\"weights w:\", w)\n",
    "print(\"bias vector b:\", b)\n",
    "print(\"linear superposition z:\", z)\n",
    "print(\"network output y_out:\", y_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMfY7VTBR1rK"
   },
   "source": [
    "### 1.2 More compact\n",
    "\n",
    "Still stay with the simple network, but define a function that evaluates the network, and visualize the output for various inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1626874872832,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "8ahzBFksR1rL",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def apply_basic_net(y_in):\n",
    "    \"\"\" Function that applies the network\n",
    "    \"\"\"\n",
    "    global w, b  # let's use global variables. We could pass them as arguments\n",
    "\n",
    "    z = np.dot(w, y_in) + b\n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with a different number of neurons\n",
    "\n",
    "> **NOTE**: NNs with 2 input neurons and 1 output neuron are ideal to illustrate how it\n",
    "works, because it allows us to visualize its behavior in a single picture. Let's see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1626874886437,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "tzGCtU9uR1rL",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N0 = 2  # input layer size\n",
    "N1 = 1  # output layer size\n",
    "\n",
    "w = np.random.uniform(low=-20, high=+20, size=(N1,N0))  # random weights: N1xN0\n",
    "b = np.random.uniform(low=-1, high=+1, size=N1)         # biases: N1 vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a valid input array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1626874895575,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "CsE8xEuPR1rL",
    "outputId": "e3396801-1459-4a7e-b070-ff414f33aec9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_in = [  ]  # <- Fill this array with random values: which size should it has?\n",
    "apply_basic_net(y_in)  # a simple test with an arbitrary input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Q**] Which shape must `y_in` have? And the output? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Multiple inputs\n",
    "\n",
    "Let's see how the network acts with many different inputs `y_in`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1626874936164,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "kFEaqMXaR1rM",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "M = 50  # Number of inputs to explore\n",
    "y_out = np.zeros([M, M]) # array MxM, to hold the result\n",
    "\n",
    "for j1 in range(M): \n",
    "    for j2 in range(M):\n",
    "        # We will explore the input space in the range [-0.5, +0.5] in 2D\n",
    "        value0 = float(j1) / M - 0.5\n",
    "        value1 = float(j2) / M - 0.5\n",
    "        y_in = [value0, value1]\n",
    "        y_out[j1, j2] = apply_basic_net(y_in)\n",
    "        \n",
    "print(y_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is an array (50 $\\times$ 50) containing the results from all that combinations of values from -0.5 to 0.5.\n",
    "\n",
    "> **Note**: this is NOT the most efficient way to do this! (but simple). Later we will learn how to use array computation efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1149,
     "status": "ok",
     "timestamp": 1626874942725,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "kHY8ZoF-R1rM",
    "outputId": "64336e64-6e04-4ee7-d735-6e058534d52f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display image\n",
    "plt.figure(figsize=(2.5,2.5))\n",
    "plt.imshow(y_out, origin='lower', extent=(-0.5,0.5,-0.5,0.5))\n",
    "plt.colorbar()\n",
    "plt.title(r\"NN output for certain input values $y_1$ and $y_2$\", fontsize=5)\n",
    "plt.xlabel(r\"$y_2$\")\n",
    "plt.ylabel(r\"$y_1$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Q**] What represent this image?\n",
    "\n",
    "[**Q**] Which kind of image is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"weights w:\", w)\n",
    "print(\"bias vector b:\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 [EXERCISE]: Try to get an output image using another activation functions\n",
    "\n",
    "Some info about activation functions:\n",
    "\n",
    "[https://en.wikipedia.org/wiki/Activation_function](https://en.wikipedia.org/wiki/Activation_function)\n",
    "\n",
    "[https://www.analyticsvidhya.com/blog/2021/04/activation-functions-and-their-derivatives-a-quick-complete-guide](https://www.analyticsvidhya.com/blog/2021/04/activation-functions-and-their-derivatives-a-quick-complete-guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adIF2dA2R1rM"
   },
   "source": [
    "## 2 NN with one hidden layer\n",
    "\n",
    "The idea here is to have multiple weight matrices (for each pair of subsequent layers, there is one weight matrix).\n",
    "The function that \"applies a layer\", i.e. goes from one layer to the next, is essentially the same as the function evaluating the simple network above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1626874964760,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "OrOJeDuzR1rN",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def apply_basic_layer(y_in, w, b):\n",
    "    \"\"\" A function that evaluates one layer based on \n",
    "        the neuron values in the preceding layer y_in.\n",
    "        Now, we pass w and b as argument to be able \n",
    "        to accommodate them to a certain layer.\n",
    "    \"\"\"\n",
    "    z = np.dot(w, y_in) + b  # exactly like before\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still with 2 input layers and 1 output layer, in order to visualize the output with an image.\n",
    "However, we will add a hidden layer with 30 neurons. Let's see how it works.\n",
    "\n",
    "Pay attention on the array's sizes to initialize weighs and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1626874970040,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "v5JXwyNNR1rN",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N0 = 2  # input layer size\n",
    "N1 = 30 # hidden layer size\n",
    "N2 = 1  # output layer size\n",
    "\n",
    "# weights and biases\n",
    "# from input layer to hidden layer:\n",
    "w1 = np.random.uniform(low=-10, high=+10, size=(N1,N0))  # random weights: N1xN0\n",
    "b1 = np.random.uniform(low=-1, high=+1, size=N1)         # biases: N1 vector\n",
    "\n",
    "# weights and biases from hidden layer to output layer:\n",
    "w2 = np.random.uniform(low=-5, high=+5, size=(N2,N1))  # random weights: N2xN1\n",
    "b2 = np.random.uniform(low=-1, high=+1, size=N2)       # biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's apply every layer of our network using the function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1626874973852,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "Cz1yN_CPR1rO",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def apply_basic_net(y_in):\n",
    "    \"\"\" Evaluate the network by subsequently evaluating the two steps \n",
    "        (input to hidden and hidden to output)\n",
    "    \"\"\"\n",
    "    global w1, b1, w2, b2\n",
    "    \n",
    "    y1 = apply_basic_layer(y_in, w1, b1)\n",
    "    y2 = apply_basic_layer(y1, w2, b2)\n",
    "    \n",
    "    return y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, obtain values for a range of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 218,
     "status": "ok",
     "timestamp": 1626874983951,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "6Tdm8QPIR1rO",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "M = 300  # number of inputs to explore\n",
    "y_out = np.zeros([M, M])  # array MxM, to hold the result\n",
    "\n",
    "t0 = time()  # initial time\n",
    "for j1 in range(M):\n",
    "    for j2 in range(M):\n",
    "        value0 = float(j1) / M - 0.5\n",
    "        value1 = float(j2) / M - 0.5\n",
    "        y_in = [value0, value1]\n",
    "        y_out[j1, j2]=apply_basic_net(y_in)\n",
    "\n",
    "print(\"Elapsed time: %.2f seconds for %d inputs.\" % (time()-t0, M**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Again, this is NOT the most efficient way to do this! (but simple) Let's continue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 790
    },
    "executionInfo": {
     "elapsed": 764,
     "status": "ok",
     "timestamp": 1626875087549,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "R40sBA-3R1rO",
    "outputId": "591f9ce8-c86d-4f6c-a798-5e4ef433788b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display image\n",
    "plt.figure(figsize=(2.5, 2.5))\n",
    "plt.imshow(y_out, origin='lower', extent=(-0.5,0.5,-0.5,0.5))\n",
    "plt.colorbar()\n",
    "plt.title(\"NN output for certain input values\", fontsize=5)\n",
    "plt.xlabel(r\"$y_2$\")\n",
    "plt.ylabel(r\"$y_1$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZCUJY6-R1rP"
   },
   "source": [
    "Obviously, the shape of the output is more 'complex' that of a simple network without any\n",
    "hidden layer. Let's go further in that direction...\n",
    "\n",
    "[**Q**] What differences are there between this image and the previous one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7oWq_SPR1rP"
   },
   "source": [
    "## 3 'batch' processing of Neural Networks\n",
    "\n",
    "As we said before, the code above is not efficient.\n",
    "We are looping over all the input values,\n",
    "and this is not the most efficient way to do it.\n",
    "We will see how to do it in a more efficient way, using 'batch' processing.\n",
    "\n",
    "Goal: apply a network to many samples in parallel using array computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhfXVsoxR1rP"
   },
   "source": [
    "### 3.1 Matrix-vector multiplication of array dimensions\n",
    "\n",
    "See how the dot product works in a no hidden layer NN (or a single step from one layer to the next in a more deep NN).\n",
    "\n",
    "For instance, let's see with $N_0=8$ (input neurons), $N_1=7$ (output neurons) and $M=50$ (different inputs).\n",
    "The idea is to have an extra dimension for the $y_{in}$ to hold the different inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2777,
     "status": "ok",
     "timestamp": 1626875178917,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "bojqaDqrR1rP",
    "outputId": "64950c99-8a19-424d-a447-f6f5c1a4b44f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N0 = 8\n",
    "N1 = 7\n",
    "M = 50\n",
    "\n",
    "W = np.zeros([N1, N0])  # Weight is N1xN0, like before\n",
    "y = np.zeros([N0, M])   # Let's define a matrix to hold the M different inputs\n",
    "\n",
    "(np.dot(W, y)).shape    # Let's see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great it seems we are in the right way.\n",
    "Now, let's try to add the bias vector entries, in the most naive way (beware!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "executionInfo": {
     "elapsed": 263,
     "status": "error",
     "timestamp": 1626875182252,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "6eTZn4pYR1rQ",
    "outputId": "4e807f81-5951-4792-a7bb-779e5dbb8bdb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "B = np.zeros(N1)  # Bias size is corresponding with the output neurons\n",
    "(np.dot(W, y) + B).shape  # <- will produce an error! Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Q**] Why is that error? How can we avoid it?\n",
    "\n",
    "Ok, we have to think a bit more...\n",
    "\n",
    "Let's play with the order of the indices (arrays shapes).\n",
    "\n",
    "Let's try to put the $M$ to the very beginning to hold the different inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1626875213771,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "MlopEfpER1rQ",
    "outputId": "7a127d70-359b-4340-ec05-129f8ce2bf2e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y = np.zeros([M, N0])\n",
    "(np.dot(y, W)).shape  # will produce an error! Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have exploited out earlier... let's think\n",
    "\n",
    "We can transpose the weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.zeros([N0, N1])\n",
    "(np.dot(y, W)).shape  # Fixed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add again the bias vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 223,
     "status": "ok",
     "timestamp": 1626875227687,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "eB4MSTDpR1rQ",
    "outputId": "b74d153f-8e19-4d45-ad3d-45e352cda7fa",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "B = np.zeros(N1)\n",
    "(np.dot(y, W) + B).shape  # Voila!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the desired result, which contains 50 results for every 7 output neurons, calculated at once.\n",
    "In this case, it is just full of zeros, but we have asserted that the method in this way is consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvu0gA4nR1rQ"
   },
   "source": [
    "### 3.2 Defining functions to evaluate a layer, and a network with batch processing\n",
    "\n",
    "Set up for batch processing, i.e. parallel evaluation of many input samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1626875241146,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "WDnsidUPR1rR",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def apply_layer_batch(y_in, w, b):\n",
    "    \"\"\" Function to apply a layer, just as before but ready for batch processing\n",
    "    \"\"\"\n",
    "    z = np.dot(y_in, w) + b  # notice the different order in matrix product!\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 230,
     "status": "ok",
     "timestamp": 1626875244101,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "ee2uQFPYR1rR",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def apply_net_batch(y_in): # exactly same as before\n",
    "    global w1, b1, w2, b2\n",
    "    \n",
    "    y1 = apply_layer_batch(y_in, w1, b1)\n",
    "    y2 = apply_layer_batch(y1, w2, b2)\n",
    "    \n",
    "    return y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a NN model. Note that it is independent of the batch! It just\n",
    "allows batching with the new ordering of the matrix indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1626875246429,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "f4jS8Uu0R1rR",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N0 = 2  # input layer size\n",
    "N1 = 30  # hidden layer size\n",
    "N2 = 1  # output layer size\n",
    "\n",
    "# from input layer to hidden layer:\n",
    "w1 = np.random.uniform(low=-10, high=+10, size=(N0,N1)) # NEW ORDER!! N0xN1\n",
    "b1 = np.random.uniform(low=-1, high=+1, size=N1)\n",
    "\n",
    "# from hidden layer to output layer:\n",
    "w2 = np.random.uniform(low=-5, high=+5, size=(N1, N2)) # NEW ORDER N1xN2\n",
    "b2 = np.random.uniform(low=-1, high=+1, size=N2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an input with a certain batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1626875247989,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "G2b6UfwkR1rR",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batchsize = 90000\n",
    "y = np.random.uniform(low=-1, high=1, size=(batchsize, N0))  # batchsize x N0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1626875249135,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "FngEZZD0R1rS",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "y_out = apply_net_batch(y)\n",
    "print(f\"Elapsed time: {time()-t0} seconds for {batchsize} inputs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1626875250087,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "750oqszIR1rS",
    "outputId": "ccc05ddf-2880-42bf-c3cb-a9663823feab",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.shape(y_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These were 90,000 samples evaluated in parallel!!! (less than 0.1 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jt56KtwSR1rS"
   },
   "source": [
    "### 3.3 Now visualize this multi-layer net, now more efficiently!\n",
    "\n",
    "Before, we had a two nested loops running all possible values for each input neuron.\n",
    "This was inefficient, but easy to calculate every combination of input values.\n",
    "Now, we have to think a way to put all that possibilities in a batch array to calculate them at once.\n",
    "How to do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 223,
     "status": "ok",
     "timestamp": 1626875258337,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "J48VP4KBR1rS",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "M = 300\n",
    "# Generate a 'mesh grid', i.e. x,y values in an ordered way. Let's see\n",
    "values = np.linspace(-0.5, 0.5, M)  # This crates an array from -0.5 to 0.5 of M elements\n",
    "xx, yy = np.meshgrid(values, values)  # Check meshgrid documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "executionInfo": {
     "elapsed": 610,
     "status": "ok",
     "timestamp": 1626875261064,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "XlczAYqkR1rT",
    "outputId": "601b6849-b593-4033-8b68-9a30839288cb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].imshow(xx, origin='lower')\n",
    "ax[0].set_title(\"input value xx\")\n",
    "im = ax[1].imshow(yy, origin='lower')\n",
    "ax[1].set_title(\"input value yy\")\n",
    "fig.colorbar(im, ax=fig.get_axes())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look, `xx` increases with the horizontal direction and remains constant on the\n",
    "vertical, like the $x$ coordinate. While `yy` runs just on the opposite,\n",
    "like $y$ coordinate. Then, we can use `xx` and `xx` just like simple Cartesian coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1626875272772,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "odaw2KOMR1rT",
    "outputId": "77d7659e-f0d4-4317-dafe-8496bbf522ba",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xx_flat = xx.flatten()  # make 1D arrays out of 2D array\n",
    "yy_flat = yy.flatten()\n",
    "# that means: MxM matrix becomes M^2 vector\n",
    "print('xx_flat: ', xx_flat, ' ; shape: ', xx_flat.shape)\n",
    "print('yy_flat: ', yy_flat, ' ; shape: ', yy_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 225,
     "status": "ok",
     "timestamp": 1626875274972,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "BudbyrMVR1rT",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batchsize = xx_flat.size  # number of samples\n",
    "y_in = np.zeros([batchsize,2])  # initialize\n",
    "y_in[:,0] = xx_flat # fill first component (index 0) -> first input neuron\n",
    "y_in[:,1] = yy_flat # fill second component (index 1) -> second input neuron\n",
    "y_in.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply net to all these samples simultaneously! At the end, this is equivalent to the `apply_net` in the double loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1626875276327,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "8eRZpeBMR1rT",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "y_out=apply_net_batch(y_in) \n",
    "print(\"Elapsed time: %f seconds for %d inputs.\" % (time()-t0, batchsize))\n",
    "y_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is not a vector, but a funny flattened matrix of batchsize x 1. Why is that size?\n",
    "\n",
    "Let's go back from a flattened array to a real matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1626875278579,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "DWWzQ4x5R1rU",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_2D = np.reshape(y_out[:,0],[M,M]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 790
    },
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1626875281319,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "pM1hFhirR1rU",
    "outputId": "47d3a65a-63d3-4057-d079-a7153205f3a0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(y_2D, origin='lower')\n",
    "plt.title(\"NN output (one hidden layer)\")\n",
    "plt.xlabel(\"xx\")\n",
    "plt.ylabel(\"yy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLUulorJR1rU"
   },
   "source": [
    "### 3.4 A network with MANY hidden layers (same size each)\n",
    "\n",
    "We can create a single `weights` hyper-matrix containing the weight of all hidden layers.\n",
    "It can be seen as a book with several sheets,\n",
    "where every sheet (slice) is a matrix corresponding to a certain weight matrix for that hidden layer.\n",
    "\n",
    "We will do the same for the `biases`. Before, it's a vector, then it becomes a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 222,
     "status": "ok",
     "timestamp": 1626875292158,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "W2_okOLTR1rU",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Nlayers = 20     # not counting the input layer nor the output layer\n",
    "LayerSize = 100  # size of each hidden layer\n",
    "\n",
    "# Note the extra dim at first for the weights and biases\n",
    "weights = np.random.uniform(low=-3, high=3, size=[Nlayers,LayerSize,LayerSize]) \n",
    "biases = np.random.uniform(low=-1, high=1, size=[Nlayers,LayerSize])\n",
    "\n",
    "# for the first layer having 2 inputs\n",
    "weightsFirst = np.random.uniform(low=-1, high=1, size=[2,LayerSize])  # see the 2\n",
    "biasesFirst = np.random.uniform(low=-1, high=1, size=LayerSize)\n",
    "\n",
    "# for the final layer having 1 output\n",
    "weightsFinal = np.random.uniform(low=-1, high=1, size=[LayerSize,1])  #see the 1\n",
    "biasesFinal = np.random.uniform(low=-1, high=1, size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1626875295792,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "WzujiQ6ER1rU",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def apply_multi_net(y_in):\n",
    "    global weights, biases, weightsFinal, biasesFinal, Nlayers\n",
    "    \n",
    "    y = apply_layer_batch(y_in, weightsFirst, biasesFirst)    \n",
    "    for j in range(Nlayers):\n",
    "        y = apply_layer_batch(y, weights[j,:,:], biases[j,:])  # We take the j-th slice\n",
    "    output = apply_layer_batch(y, weightsFinal, biasesFinal)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a set of pair-values (inputs) using a flattened meshgrid, as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1626875297291,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "kbIE7_qWR1rV",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate a 'mesh grid', i.e. x,y values in an image\n",
    "M = 300  # Let's do it with a quite high resolution\n",
    "xx, yy = np.meshgrid(np.linspace(-0.5, 0.5, M), np.linspace(-0.5, 0.5, M))\n",
    "batchsize = M**2  # number of samples = M^2  (it's equivalent as before)\n",
    "\n",
    "y_in = np.zeros([batchsize, 2])  # initialize\n",
    "y_in[:,0] = xx.flatten() # fill first component (index 0)\n",
    "y_in[:,1] = yy.flatten() # fill second component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this dens NN in batch mode (300 $\\times$ 300 = 90,000 different inputs at once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 246,
     "status": "ok",
     "timestamp": 1626875298627,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "kaPZGeWNR1rV",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "y_out = apply_multi_net(y_in)  # apply net to all these samples! (it takes some seconds)\n",
    "print(\"Elapsed time: %f seconds for %d inputs.\" % (time()-t0, batchsize))\n",
    "y_2D = np.reshape(y_out, [M,M]) # back to 2D image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "executionInfo": {
     "elapsed": 441,
     "status": "ok",
     "timestamp": 1626875301384,
     "user": {
      "displayName": "David Maluenda",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWqrU3JdB-d8lhWcsydkhDTsyMGlOXfplagiOHaw=s64",
      "userId": "14097055138958710373"
     },
     "user_tz": -120
    },
    "id": "vKFIzkmKR1rW",
    "outputId": "eb87c9b9-8382-456c-c26a-c8a0243918af",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(y_2D, origin='lower', extent=[-0.5,0.5,-0.5,0.5])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Q**] Why does this image look like this?\n",
    "\n",
    "[**Q**] What are hidden layers introducing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Summing up in general functions\n",
    "\n",
    "Letâs define a couple of functions to create an arbitrary deep and fully connected neural network in batch mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_layer(y_in, w, b, activation):\n",
    "    \"\"\" Batch processing of a single layer:\n",
    "           y_in: input values  -> shape: [batchsize, num_neurons_in]\n",
    "           w:    weight matrix -> shape: [n_neurons_in, n_neurons_out]\n",
    "           b:    bias vector   -> length: n_neurons_out\n",
    "    \n",
    "           activation is some string of the following ones:\n",
    "             - sigmoid\n",
    "             - jump\n",
    "             - linear\n",
    "             - reLU\n",
    "    \n",
    "           returns the values of the output neurons in the next layer \n",
    "              -> shape: [batchsize, n_neurons_out]\n",
    "    \"\"\"\n",
    "    z = np.dot(y_in, w) + b\n",
    "    if activation == 'sigmoid':\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    elif activation == 'jump':\n",
    "        return np.array(z>0, dtype='float')\n",
    "    elif activation == 'linear':\n",
    "        return z\n",
    "    elif activation == 'reLU':\n",
    "        return (z > 0) * z\n",
    "\n",
    "    \n",
    "def apply_net(y_in, weights, biases, activations):\n",
    "    \"\"\" Apply a whole network of multiple layers.\n",
    "          y_in: input values  -> shape: [batchsize, num_neurons_in]\n",
    "          weights, biases and activations must be any iterable \n",
    "          which length is the layers' number containing\n",
    "              weight matrix  -> shape: [n_neurons_in, n_neurons_out]\n",
    "              bias vector    -> length: n_neurons_out\n",
    "              activation str -> sigmoid, jump linear or reLU\n",
    "          Alternatively, they can be extended matrices\n",
    "          where a simple slicing generates the proper weight, \n",
    "          bias and activation.\n",
    "    \"\"\"\n",
    "    y = y_in\n",
    "    for j in range(len(biases)):\n",
    "        y = apply_layer(y, weights[j], biases[j], activations[j])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Fancy visualization of Neural Networks with Pure Python\n",
    "\n",
    "### 5.1 Some internal routines for plotting the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLUE_COLOR = [0, 0.4, 0.8]  # RGB color\n",
    "ORANGE_COLOR = [1, 0.3, 0]  # RGB color\n",
    "\n",
    "def plot_connection_line(ax, X, Y, W, vmax=1.0, linewidth=3):\n",
    "    \"\"\" Draw a fancy line from (X[0], Y[0]) to (X[1], Y[1])\n",
    "        according to the weight W into the frame ax.\n",
    "    \"\"\"\n",
    "    t = np.linspace(0,1,20)  # free parameter to draw lines\n",
    "    \n",
    "    if W > 0:  # Color depending on the weight's sign\n",
    "        col = BLUE_COLOR\n",
    "    else:\n",
    "        col = ORANGE_COLOR\n",
    "    \n",
    "    # fancy line from (X0, Y0) to (X1, Y1)\n",
    "    xx = X[0] + t*(X[1] - X[0])  # Linear in horizontal\n",
    "    yy = Y[0] + (3*t**2 - 2*t**3) * (Y[1] - Y[0])  # Round borders\n",
    "    \n",
    "    # plotting the line according to the weight\n",
    "    ax.plot(xx, yy, alpha=abs(W)/vmax,\n",
    "            color=col, linewidth=linewidth)\n",
    "\n",
    "    \n",
    "def plot_neuron_alpha(ax, X, Y, B, size=100.0, vmax=1.0):\n",
    "    \"\"\" Draw a single neuron in position (X, Y) according to \n",
    "        the bias B, into the frame ax.\n",
    "    \"\"\"\n",
    "    if B > 0:\n",
    "        col = BLUE_COLOR\n",
    "    else:\n",
    "        col = ORANGE_COLOR\n",
    "        \n",
    "    ax.scatter([X], [Y], marker='o', c=col, alpha=abs(B)/vmax, \n",
    "               s=size, zorder=10)\n",
    "\n",
    "    \n",
    "def plot_neuron(ax, X, Y, B, size=100.0, vmax=1.0):\n",
    "    \"\"\" Draw a single neuron in position (X, Y) independently to \n",
    "        the bias B, into the frame ax.\n",
    "    \"\"\"\n",
    "    if B > 0:\n",
    "        col = BLUE_COLOR\n",
    "    else:\n",
    "        col = ORANGE_COLOR\n",
    "        \n",
    "    ax.scatter([X], [Y], marker='o', c=col, s=size, zorder=10)\n",
    "    \n",
    "    \n",
    "def visualize_network(weights, biases, activations, M=100,\n",
    "                      y0range=[-1,1], y1range=[-1,1],\n",
    "                      size=400.0, linewidth=5.0):\n",
    "    \"\"\"\n",
    "    Visualize a neural network with 2 input \n",
    "    neurons and 1 output neuron (plot output vs input in a 2D plot)\n",
    "    \n",
    "    weights is a list of the weight matrices for the\n",
    "    layers, where weights[j] is the matrix for the connections\n",
    "    from layer j to layer j+1 (where j==0 is the input)\n",
    "    \n",
    "    weights[j][m,k] is the weight for input neuron k going to output neuron m\n",
    "    (note: internally, m and k are swapped, see the explanation of\n",
    "    batch processing in lecture 2)\n",
    "    \n",
    "    biases[j] is the vector of bias values for obtaining the neurons \n",
    "    in layer j+1, biases[j][k] is the bias for neuron k in layer j+1\n",
    "    \n",
    "    activations is a list of the activation functions for\n",
    "    the different layers: choose 'linear','sigmoid', \n",
    "    'jump' (i.e. step-function), and 'reLU'\n",
    "    \n",
    "    M is the resolution (MxM grid)\n",
    "    \n",
    "    y0range is the range of y0 neuron values (horizontal axis)\n",
    "    y1range is the range of y1 neuron values (vertical axis)\n",
    "    \"\"\"\n",
    "    # Let's transpose the weight to be able the batch processing\n",
    "    swapped_weights = []\n",
    "    for j in range(len(weights)):\n",
    "        swapped_weights.append(np.transpose(weights[j]))\n",
    "        \n",
    "    # Let's create a set of input-pairs by means of a mesh grid\n",
    "    y0, y1 = np.meshgrid(np.linspace(y0range[0], y0range[1], M),\n",
    "                         np.linspace(y1range[0], y1range[1], M))\n",
    "    y_in = np.zeros([M*M, 2])\n",
    "    y_in[:, 0] = y0.flatten()\n",
    "    y_in[:, 1] = y1.flatten()\n",
    "    \n",
    "    # Let's apply the NN\n",
    "    y_out = apply_net(y_in, swapped_weights, biases, activations)\n",
    "\n",
    "    # We will plot a diagram at left and the result at right\n",
    "    fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(8,4))\n",
    "    \n",
    "    \n",
    "    # For the diagram\n",
    "    \n",
    "    #  1: posX and posY are arrays containing the positions of neurons\n",
    "    posX = [[0, 0]]  # same column for both (at left)    \n",
    "    posY = [[-0.5, +0.5]]  # for 2 inputs, let's putted centered in high\n",
    "\n",
    "    \n",
    "    vmax = 0.0 # for finding the maximum weight\n",
    "    vmaxB = 0.0 # for maximum bias\n",
    "    for j in range(len(biases)):  # for every layer on the NN\n",
    "        n_neurons = len(biases[j])  # neurons in the current layer\n",
    "        \n",
    "        posX.append(np.full(n_neurons, j+1))  # next column to the previous one\n",
    "        posY.append(np.array(range(n_neurons)) - 0.5 * (n_neurons-1)) # spread\n",
    "        \n",
    "        vmax = np.maximum(vmax, np.max(np.abs(weights[j])))  # to get the maximum\n",
    "        vmaxB = np.maximum(vmaxB, np.max(np.abs(biases[j])))\n",
    "\n",
    "    #   2: plot connections\n",
    "    for j in range(len(biases)):  # for each layer\n",
    "        for k in range(len(posX[j])):  # for each neuron\n",
    "            for m in range(len(posX[j+1])):  # for each following neuron\n",
    "                plot_connection_line(ax[0],  # first column of the plot\n",
    "                                     [posX[j][k], posX[j+1][m]], # [X0,X1]\n",
    "                                     [posY[j][k], posY[j+1][m]], # [Y0,Y1]\n",
    "                                     swapped_weights[j][k,m],    # its weight\n",
    "                                     vmax=vmax,  # to get normalized plots\n",
    "                                     linewidth=linewidth)\n",
    "    \n",
    "    #   3: plot neurons\n",
    "    for k in range(len(posX[0])):  # input neurons (have no bias!)\n",
    "        plot_neuron(ax[0], posX[0][k], posY[0][k],\n",
    "                    vmaxB, vmax=vmaxB, size=size)\n",
    "        \n",
    "    for j in range(len(biases)): # all other neurons\n",
    "        for k in range(len(posX[j+1])):\n",
    "            plot_neuron_alpha(ax[0], posX[j+1][k], posY[j+1][k],\n",
    "                              biases[j][k], vmax=vmaxB, size=size)\n",
    "    \n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    # now: the output of the network\n",
    "    img = ax[1].imshow(np.reshape(y_out, [M,M]),\n",
    "                       origin='lower',\n",
    "                       extent=[y0range[0],y0range[1],y1range[0],y1range[1]])\n",
    "    ax[1].set_xlabel(r'$y_0$')\n",
    "    ax[1].set_ylabel(r'$y_1$')\n",
    "    \n",
    "    axins1 = inset_axes(ax[1],\n",
    "                        width=\"40%\",  # width = 50% of parent_bbox width\n",
    "                        height=\"5%\",  # height : 5%\n",
    "                        loc='upper right')\n",
    "\n",
    "    imgmin = np.min(y_out)\n",
    "    imgmax = np.max(y_out)\n",
    "    color_bar = fig.colorbar(img, cax=axins1, orientation=\"horizontal\",\n",
    "                             ticks=np.linspace(imgmin,imgmax,3))\n",
    "    cbxtick_obj = plt.getp(color_bar.ax.axes, 'xticklabels')\n",
    "    plt.setp(cbxtick_obj, color=\"white\")\n",
    "    axins1.xaxis.set_ticks_position(\"bottom\")\n",
    "\n",
    "    ax[1].set_title(activations[0])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 No hidden layer NN\n",
    "\n",
    "Let's visualize a simple network (no hidden layer) with different activation functions.\n",
    "\n",
    "Notice that no hidden layer means that weight, biases and activations are list of length 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANGE = [-3, 3]  # all tests will be with the same range\n",
    "\n",
    "weights=[ [      # a list of matrices (length 1 in this case)\n",
    "    [-0.2, 0.9]  # from 2 input neurons to a single output neuron: 1x2\n",
    "    ] ]\n",
    "\n",
    "biases=[   # a list of vectors (length 1 in this case)\n",
    "    [0.5]  # bias for 1 single output neuron: 1 value\n",
    "    ]\n",
    "\n",
    "visualize_network(weights, biases, ['sigmoid'],\n",
    "                  y0range=RANGE, y1range=RANGE)\n",
    "\n",
    "visualize_network(weights, biases, ['jump'],\n",
    "                  y0range=RANGE, y1range=RANGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Deep dense NN\n",
    "\n",
    "Let's visualize a NN of 1 hidden layer of 3 neurons using different activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weights = [ [  # a list of matrices (length 2 in this case)\n",
    "        [0.2, 0.9],  # hidden layer of 3 neurons\n",
    "        [-0.5, 0.3], #  -> 3 neurons with 2 inputs: matrix of 3x2\n",
    "        [0.8, -1.3]\n",
    "    ],                 \n",
    "    [ \n",
    "        [-0.3,0.7,0.5] # from 3 hidden neurons to one output: 1x3\n",
    "    ]  ]\n",
    "\n",
    "biases=[  # a list of vectors (length 2 in this case)\n",
    "        [0.1, -0.5, -0.5], # biases of 3 hidden neurons\n",
    "        [-0.2] # bias for output neuron\n",
    "       ]\n",
    "\n",
    "visualize_network(weights, biases, ['sigmoid', 'sigmoid'],\n",
    "                  y0range=RANGE, y1range=RANGE)\n",
    "\n",
    "visualize_network(weights, biases, ['jump', 'jump'],\n",
    "                  y0range=RANGE, y1range=RANGE)\n",
    "\n",
    "visualize_network(weights, biases, ['reLU', 'reLU'],\n",
    "                  y0range=RANGE, y1range=RANGE)\n",
    "\n",
    "visualize_network(weights, biases, ['linear', 'linear'],\n",
    "                  y0range=RANGE, y1range=RANGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "[**Q**] Some question?  *** *** ***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More layers and some activation function combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now with 5 intermediate neurons, for fun:\n",
    "\n",
    "weights = [ [  # list of matrices\n",
    "                  [0.2, 0.9],  # 5x2\n",
    "                  [-0.5, 0.3],\n",
    "                  [0.8, -1.3],\n",
    "                  [-0.3, -0.9],\n",
    "                  [-0.8, -1.2]\n",
    "          ],\n",
    "          [ \n",
    "                  [-0.3, 0.7, 0.5, -0.3, 0.4]  # 1x5\n",
    "          ] ]\n",
    "\n",
    "biases = [  # list of vectors\n",
    "                [0.1, -0.5, -0.5, 0.3, 0.2],  # bias for the hidden layer\n",
    "                [0.5]  # bias for the output bias\n",
    "         ]   # bias for the output neuron\n",
    "\n",
    "visualize_network(weights, biases,              \n",
    "                  activations=[ 'jump', 'linear' ],\n",
    "                  y0range=RANGE, y1range=RANGE, M=400)\n",
    "\n",
    "visualize_network(weights, biases,              \n",
    "                  activations=[ 'sigmoid', 'linear' ],\n",
    "                  y0range=RANGE, y1range=RANGE, M=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "[**Q**] Some question?  *** *** ***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sharper that sigmoid shape: scale all weights and biases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor=10.0  # as high as sharpen \n",
    "\n",
    "# this needs np.array(), because you cannot do factor*<python-list>\n",
    "ws = [factor*np.array(matrix) for matrix in weights]\n",
    "bs = [factor*np.array(vector) for vector in biases]\n",
    "\n",
    "visualize_network(ws, bs, activations=[ 'sigmoid', 'linear' ],\n",
    "                  y0range=RANGE, y1range=RANGE, M=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "[**Q**] Some question?  *** *** ***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Something not random\n",
    "\n",
    "Many superimposed lines can be used to construct arbitrary shapes, with only a single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 10  # factor to sharpen sigmoid\n",
    "n_lines = 3  # number of lines to draw\n",
    "\n",
    "phi = np.linspace(0, 2*np.pi, n_lines+1)  # Angular variable\n",
    "phi = phi[0:-1]  # the last value is 2pi, which is equivalent to the 0\n",
    "\n",
    "weight_hidden = np.zeros([n_lines, 2])   # comment this shape\n",
    "weight_hidden[:,0] = factor*np.cos(phi)  # x=cos(phi)\n",
    "weight_hidden[:,1] = factor*np.sin(phi)  # y=sin(phi)\n",
    "\n",
    "bias_hidden = np.full(n_lines, factor*(+0.5))  # all neurons acts equally\n",
    "\n",
    "visualize_network(weights=[ \n",
    "                            weight_hidden,           # from input to hidden\n",
    "                            np.full([1,n_lines],1.0) # from hidden to output\n",
    "                          ],\n",
    "                  biases=[ \n",
    "                            bias_hidden,\n",
    "                            [0.0]\n",
    "                         ],\n",
    "                  activations=['sigmoid',  # activation for hidden\n",
    "                               'linear'    # activation for output\n",
    "                              ],\n",
    "                  y0range=RANGE, y1range=RANGE,\n",
    "                  size=30.0, M=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Q**] Play with different `factor` above to see how sigmoid behavior changes.\n",
    "\n",
    "[**Q**] `n_lines` sets the number of lines on the figure, but what does it represent in the NN? Play with different number of lines.\n",
    "\n",
    "[**Q**] Why weights are made of sines and cosines?\n",
    "\n",
    "[**Q**] What are biases here? Play with it.\n",
    "\n",
    "[**Q**] Why the weights corresponding from hidden to output layer are full of ones?\n",
    "\n",
    "[**Q**] Why the output's activation function is linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.4.1 [EXERCISE] Draw a sharp and big six-pointed star using the code above"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4.2 [EXERCISE] Draw a blurred and small circle using the code above"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "name": "01_Basics_NeuralNetworksPython.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {
    "height": "326px",
    "width": "580px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "437.143px",
    "left": "122px",
    "top": "121.143px",
    "width": "291px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "370.837px",
    "left": "987px",
    "right": "20px",
    "top": "120px",
    "width": "530px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
