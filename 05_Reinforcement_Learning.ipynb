{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"text-align: right\"><sub>This notebook is distributed under the <a href=\"https://creativecommons.org/licenses/by-sa/4.0/\" target=\"_blank\">Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license</a>.</sub></div>\n",
    "<h1>Hands on Machine Learning  <span style=\"font-size:12px;\"><i>by <a href=\"https://webgrec.ub.edu/webpages/000004/cat/dmaluenda.ub.edu.html\" target=\"_blank\">David Maluenda</a></i></span></h1>\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://atenea.upc.edu/course/view.php?id=71605\" target=\"_blank\">\n",
    "      <img src=\"https://github.com/dmaluenda/hands_on_machine_learning/raw/master/resources/upc_logo_49px.png\" width=\"130\"/>\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "  </td>\n",
    "  <td>   <!-- gColab -->\n",
    "    <a href=\"https://colab.research.google.com/github/dmaluenda/hands_on_machine_learning/blob/master/05_Reinforcement_Learning.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/colab_logo_32px.png\" />\n",
    "      Run in Google Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- github -->\n",
    "    <a href=\"https://github.com/dmaluenda/hands_on_machine_learning/blob/master/05_Reinforcement_Learning.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/github_logo_32px.png\" />\n",
    "      View source on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>   <!-- download -->\n",
    "    <a href=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/05_Reinforcement_Learning.ipynb\"  target=\"_blank\"\n",
    "          download=\"03_Reinforcement_Learning\">\n",
    "      <img src=\"https://raw.githubusercontent.com/dmaluenda/hands_on_machine_learning/master/resources/download_logo_32px.png\" />\n",
    "      Download notebook\n",
    "      </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# $\\text{V}$. Reinforcement Learning\n",
    "\n",
    "Hands on \"Machine Learning on Classical and Quantum data\" course of\n",
    "[Master in Photonics - PHOTONICS BCN](https://photonics.masters.upc.edu/en/general-information)\n",
    "[[UPC](https://photonics.masters.upc.edu/en) +\n",
    "[UB](https://www.ub.edu/web/ub/en/estudis/oferta_formativa/master_universitari/fitxa/P/M0D0H/index.html?) +\n",
    "[UAB](https://www.uab.cat/en/uab-official-masters-degrees-study-guides/) +\n",
    "[ICFO](https://www.icfo.eu/lang/studies/master-studies)].\n",
    "\n",
    "Tutorial 5\n",
    "\n",
    "This notebook shows how to:\n",
    "- solve a challenge with reinforcement learning\n",
    "- implement the Actor-Critic method\n",
    "- implement the Deep Deterministic Policy Gradient\n",
    "\n",
    "**References**:\n",
    "\n",
    "[1] [Keras](https://keras.io/getting_started/): a deep learning API written in Python.<br>\n",
    "[2] [Keras examples](https://keras.io/examples/rl/).<br>\n",
    "[3] Gym, a toolkit for developing and comparing reinforcement learning algorithms. By [OpenAI](https://gym.openai.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#0.-Imports:-Basically-numpy,-matplotlib,-tensorflow-and-gym\" data-toc-modified-id=\"0.-Imports:-Basically-numpy,-matplotlib,-tensorflow-and-gym-0\">0. Imports: Basically numpy, matplotlib, tensorflow and gym</a></span></li><li><span><a href=\"#1.-Actor-Critic-Method-(CartPole-game-from-OpenAI-gym)\" data-toc-modified-id=\"1.-Actor-Critic-Method-(CartPole-game-from-OpenAI-gym)-1\">1. Actor Critic Method (CartPole game from OpenAI gym)</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-Configuration-parameters-for-the-whole-setup\" data-toc-modified-id=\"1.1-Configuration-parameters-for-the-whole-setup-1.1\">1.1 Configuration parameters for the whole setup</a></span></li><li><span><a href=\"#1.2-Playing-function\" data-toc-modified-id=\"1.2-Playing-function-1.2\">1.2 Playing function</a></span></li><li><span><a href=\"#1.3-Neural-network-player-(DQN)\" data-toc-modified-id=\"1.3-Neural-network-player-(DQN)-1.3\">1.3 Neural network player (DQN)</a></span></li><li><span><a href=\"#1.4-Reinforcement-training\" data-toc-modified-id=\"1.4-Reinforcement-training-1.4\">1.4 Reinforcement training</a></span></li><li><span><a href=\"#1.5-Playing\" data-toc-modified-id=\"1.5-Playing-1.5\">1.5 Playing</a></span></li><li><span><a href=\"#1.6-[EXERCISE]:-Try-with-a-different-Agent-model-(different-number-of-hidden-layers/neurons)-to-solve-the-problem-with-less-episodes.\" data-toc-modified-id=\"1.6-[EXERCISE]:-Try-with-a-different-Agent-model-(different-number-of-hidden-layers/neurons)-to-solve-the-problem-with-less-episodes.-1.6\">1.6 [EXERCISE]: Try with a different Agent model (different number of hidden layers/neurons) to solve the problem with less episodes.</a></span></li></ul></li><li><span><a href=\"#2.-Inverted-Pendulum-game-from-OpenAI-gym-(DDPG)\" data-toc-modified-id=\"2.-Inverted-Pendulum-game-from-OpenAI-gym-(DDPG)-2\">2. Inverted Pendulum game from OpenAI gym (DDPG)</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1-Deep-Deterministic-Policy-Gradient-(DDPG)\" data-toc-modified-id=\"2.1-Deep-Deterministic-Policy-Gradient-(DDPG)-2.1\">2.1 Deep Deterministic Policy Gradient (DDPG)</a></span></li><li><span><a href=\"#2.2-Useful-classes\" data-toc-modified-id=\"2.2-Useful-classes-2.2\">2.2 Useful classes</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.2.1-The-OUActionNoise-class\" data-toc-modified-id=\"2.2.1-The-OUActionNoise-class-2.2.1\">2.2.1 The OUActionNoise class</a></span></li><li><span><a href=\"#2.2.2-The-Buffer-class\" data-toc-modified-id=\"2.2.2-The-Buffer-class-2.2.2\">2.2.2 The Buffer class</a></span></li></ul></li><li><span><a href=\"#2.3-NNs-definition\" data-toc-modified-id=\"2.3-NNs-definition-2.3\">2.3 NNs definition</a></span></li><li><span><a href=\"#2.4-Policy-definition-and-training\" data-toc-modified-id=\"2.4-Policy-definition-and-training-2.4\">2.4 Policy definition and training</a></span></li><li><span><a href=\"#2.5-Playing\" data-toc-modified-id=\"2.5-Playing-2.5\">2.5 Playing</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 0. Imports: Basically numpy, matplotlib, tensorflow and gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`!pip install gym`\n",
    "\n",
    "`!pip install gym[all] `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tfy\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import gym\n",
    "\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Actor Critic Method (CartPole game from OpenAI gym)\n",
    "\n",
    "Code base on https://keras.io/examples/rl/actor_critic_cartpole/\n",
    "\n",
    "Game documentation: https://www.gymlibrary.dev/environments/classic_control/cart_pole/\n",
    "\n",
    "* **Description**\n",
    "\n",
    "    This environment corresponds to the version of the cart-pole problem\n",
    "    described by Barto, Sutton, and Anderson in [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077).\n",
    "    \n",
    "    A pole is attached by an un-actuated joint to a cart, which moves along a\n",
    "    frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n",
    "    \n",
    "\n",
    "* **Action Space**\n",
    "\n",
    "    The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction of the fixed force the cart is pushed with.\n",
    "    \n",
    "    | Num | Action                 |\n",
    "    |-----|:-----------------------|\n",
    "    | 0   | Push cart to the left  |\n",
    "    | 1   | Push cart to the right |\n",
    "\n",
    "    **Note**: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it.\n",
    "\n",
    "\n",
    "* **State Space**\n",
    "\n",
    "    The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
    "    \n",
    "    | Num | Observation           | Min                  | Max                |\n",
    "    |-----|-----------------------|----------------------|--------------------|\n",
    "    | 0   | Cart Position         | -4.8                 | 4.8                |\n",
    "    | 1   | Cart Velocity         | -Inf                 | Inf                |\n",
    "    | 2   | Pole Angle            | ~ -0.418 rad (-24°)  | ~ 0.418 rad (24°)  |\n",
    "    | 3   | Pole Angular Velocity | -Inf                 | Inf                |\n",
    "\n",
    "    **Note:** While the ranges above denote the possible values for observation space of each element, it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
    "    -  The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates if the cart leaves the `(-2.4, 2.4)` range.\n",
    "    -  The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n",
    "\n",
    "\n",
    "* **Rewards**\n",
    "\n",
    "    Since the goal is to keep the pole upright for as long as possible, a reward of `+1` for every step taken, including the termination step, is allotted. The threshold for rewards is 475 for v1.\n",
    "\n",
    "\n",
    "* **Starting State**\n",
    "\n",
    "    All observations are assigned a uniformly random value in `(-0.05, 0.05)`\n",
    "\n",
    "\n",
    "* **Episode Termination**\n",
    "\n",
    "    The episode terminates if any one of the following occurs:\n",
    "    1. Pole Angle is greater than ±12°\n",
    "    2. Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "    3. Episode length is greater than 500 (200 for v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.1 Configuration parameters for the whole setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current state: [ 0.0104645   0.02341712 -0.01145225 -0.0260633 ]\n",
      "new state: [ 0.01093284 -0.17153874 -0.01197351  0.26298442]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dmaluenda\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\dmaluenda\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:211: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"CartPole-v0\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")  # Create the environment\n",
    "# env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "print(f\"current state: {env.reset()[0]}\")\n",
    "env.render()  # This creates a figure, open it and make it visible\n",
    "\n",
    "action = 0  # move to left (or right) is 0 (or 1)\n",
    "state, reward, done, info = env.step(action)[:4]\n",
    "\n",
    "print(f\"new state: {state}\")\n",
    "print(f\"reward: {reward}\")\n",
    "print(f\"done: {done}\")\n",
    "print(f\"info: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2 Playing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def play(environ, player=None, is_discrete=True, reward_to_win=300):\n",
    "\n",
    "    cum_reward = 0\n",
    "    rewards_list = []\n",
    "    state = environ.reset()\n",
    "\n",
    "    print(\"Let's play: \", flush=True, end='')\n",
    "    environ.render()\n",
    "    sleep(1)\n",
    "    \n",
    "    while True:\n",
    "               \n",
    "        if player:  # chose an action from a given state\n",
    "            state = state[0] if type(state)==tuple else state\n",
    "            state = np.expand_dims(state, 0)\n",
    "            output = player(state)\n",
    "            \n",
    "            action = output[0]\n",
    "            if is_discrete:\n",
    "                # chose an action according to its probability\n",
    "                action = np.random.choice(num_actions, p=np.squeeze(action))\n",
    "        \n",
    "        else:  # random action\n",
    "            action = environ.action_space.sample()  # No player is random player\n",
    "        \n",
    "        # just to print the taken action\n",
    "        action_str = ('L, ' if action == 0 else 'R, ' if is_discrete \n",
    "                      else (\"%.2f, \" % action))\n",
    "        print(action_str, flush=True, end='')\n",
    "        \n",
    "        # apply an action an render it\n",
    "        state, reward, done, info = environ.step(action)[:4]\n",
    "        environ.render()\n",
    "        \n",
    "        # take note of the reward\n",
    "        cum_reward += reward\n",
    "        rewards_list.append(reward)\n",
    "        if done:  # The environ decides when its enough (game's rule)\n",
    "            print(\"Done!\")\n",
    "            final_mark = cum_reward if is_discrete else np.array(rewards_list[-40:]).mean()\n",
    "            win_lost = \"YOU WIN!\" if final_mark >= reward_to_win else \"Sorry, try again...\"\n",
    "            print(\"The total reward is %.2f.\" % final_mark, win_lost)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's play: R, L, R, L, R, L, L, L, L, R, L, L, L, R, L, R, Done!\n",
      "The total reward is 16.00. Sorry, try again...\n"
     ]
    }
   ],
   "source": [
    "play(env, reward_to_win=300)  # Random player just to see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.3 Neural network player (DQN)\n",
    "\n",
    "The Agent is a single NN with two separate output layers, one for Actor, one for Critic.\n",
    "\n",
    "* **Actor** try to decide which action is better. In this case we have to possible actions (move to left or to right), then a two softmax neurons as output is the choice.\n",
    "\n",
    "* **Critic** try to estimate the next reward, according to the current state. A single output neuron is enough.\n",
    "\n",
    "In both cases we use the same input block containing the input layer (with as many neurons like the state shape) and a hidden layer of a certain number of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_inputs = 4    # state space\n",
    "num_actions = 2   # action space\n",
    "num_hidden = 128  # free hyper-parameter\n",
    "\n",
    "inputs = layers.Input(shape=(num_inputs,))\n",
    "hidden = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(hidden)\n",
    "critic = layers.Dense(1)(hidden)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "huber_loss = keras.losses.Huber()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "More info about Huber loss: https://en.wikipedia.org/wiki/Huber_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state = np.expand_dims(state[0], 0)  # we have to add an extra dimension at first, why?\n",
    "print(\"state:\", state)\n",
    "action_probs, critic_value = model(state)\n",
    "print(\"action_probs:\", action_probs)\n",
    "print(\"critic_values:\", critic_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Why all inputs and outputs have one extra dimension in the first place?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.4 Reinforcement training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gamma = 0.99  # Discount factor for past rewards\n",
    "max_steps_per_episode = 10000\n",
    "\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "rewards_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "# eps avoid dividing by zero\n",
    "eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0\n",
    "\n",
    "while True:  # Run until solved\n",
    "    # Each loop is a new game\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    do_show = episode_count % 10 == 0  # show some progress\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        if do_show:\n",
    "            print(\"Playing episode %d...\" % episode_count, flush=True, end='')\n",
    "            \n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            \"\"\" Loop over one episode \"\"\"\n",
    "\n",
    "            if do_show:\n",
    "                env.render()\n",
    "            \n",
    "            # Predict action probabilities and estimated future rewards\n",
    "            # from environment state\n",
    "            action_probs, critic_value = model(np.expand_dims(state[0], 0))\n",
    "            critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "            # Sample action from action probability distribution\n",
    "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "            \n",
    "            # Apply the sampled action in our environment\n",
    "            state, reward, done, _ = env.step(action)[0]\n",
    "            rewards_history.append(reward)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break  # for loop over time steps of a certaon episode\n",
    "\n",
    "        # Update running reward to check condition for solving\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # Calculate expected value from rewards\n",
    "        # - At each timestep what was the total reward received after that timestep\n",
    "        # - Rewards in the past are discounted by multiplying them with gamma\n",
    "        # - These are the labels for our critic\n",
    "        returns = []  # ground truth for critic\n",
    "        discounted_sum = 0\n",
    "        for r in rewards_history[::-1]:\n",
    "            discounted_sum = r + gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "\n",
    "        # Normalize returns\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "        returns = returns.tolist()\n",
    "\n",
    "        # Calculating loss values to update our network\n",
    "        history = zip(action_probs_history, critic_value_history, returns)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, ret in history:\n",
    "            # At this point in history, the critic estimated that we would get a\n",
    "            # total reward = `value` in the future. \n",
    "            # We took an action with log probability of `log_prob` and \n",
    "            # ended up receiving a total reward = `ret`.\n",
    "            # The actor must be updated so that it predicts an action that leads \n",
    "            # to high rewards (compared to critic's estimate) with high probability.\n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "            # The critic must be updated so that it predicts a better estimate of\n",
    "            # the future rewards.\n",
    "            critic_losses.append(\n",
    "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "                    )\n",
    "\n",
    "        # Backpropagation\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Clear the loss and reward history\n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "\n",
    "    # Log details\n",
    "    episode_count += 1\n",
    "    if do_show:\n",
    "        print(\" reward of %.2f\" % running_reward, flush=True)\n",
    "\n",
    "    if running_reward > 300:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.5 Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "play(env, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.6 [EXERCISE]: Try with a different Agent model (different number of hidden layers/neurons) to solve the problem with less episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Inverted Pendulum game from OpenAI gym (DDPG)\n",
    "\n",
    "Code base on https://keras.io/examples/rl/ddpg_pendulum/\n",
    "\n",
    "Game documentation https://gym.openai.com/envs/Pendulum-v0/\n",
    "\n",
    "* **Description**\n",
    "\n",
    "    The inverted pendulum swingup problem is based on the classic problem in control theory. The system consists of a pendulum attached at one end to a fixed point, and the other end being free. The pendulum starts in a random position and the goal is to apply torque on the free end to swing it into an upright position, with its center of gravity right above the fixed point.\n",
    "    The diagram below specifies the coordinate system used for the implementation of the pendulum's\n",
    "    dynamic equations.\n",
    "    ![Pendulum Coordinate System](https://miro.medium.com/max/500/1*ygcr1p1hytn8p41ycjoSvQ.gif)\n",
    "    -  `x-y`: cartesian coordinates of the pendulum's end in meters.\n",
    "    - `theta` : angle in radians.\n",
    "    - `tau`: torque in `N m`. Defined as positive _counter-clockwise_.\n",
    "\n",
    "\n",
    "* **Action Space**\n",
    "\n",
    "    The action is a `ndarray` with shape `(1,)` representing the torque applied to free end of the pendulum.\n",
    "    | Num | Action | Min  | Max |\n",
    "    |-----|--------|------|-----|\n",
    "    | 0   | Torque | -2.0 | 2.0 |\n",
    "\n",
    "\n",
    "* **State Space**\n",
    "\n",
    "    The observation is a `ndarray` with shape `(3,)` representing the x-y coordinates of the pendulum's free end and its angular velocity.\n",
    "    | Num | Observation      | Min  | Max |\n",
    "    |-----|------------------|------|-----|\n",
    "    | 0   | x = cos(theta)   | -1.0 | 1.0 |\n",
    "    | 1   | y = sin(angle)   | -1.0 | 1.0 |\n",
    "    | 2   | Angular Velocity | -8.0 | 8.0 |\n",
    "\n",
    "\n",
    "* **Rewards**\n",
    "\n",
    "    The reward function is defined as:\n",
    "    $$r = -(\\theta^2 + 0.1 * \\omega^2 + 0.001 * \\tau^2)$$\n",
    "    where $\\theta$ is the pendulum's angle normalized between $[-\\pi, \\pi]$ (with 0 being in the upright position), $\\omega$ is the angular velocity and $\\tau$ the applied torque.\n",
    "    \n",
    "    Based on the above equation, the minimum (worst) reward that can be obtained is $$-(\\pi^2 + 0.1 * 8^2 + 0.001 * 2^2) = -16.273$$ while the maximum (best) reward is zero (pendulum is\n",
    "    upright with zero velocity and no torque applied).\n",
    "\n",
    "\n",
    "* **Starting State**\n",
    "\n",
    "    The starting state is a random angle in $[-\\pi, \\pi]$ and a random angular velocity in $[-1,1]$.\n",
    "    \n",
    "    \n",
    "* **Episode Termination**\n",
    "\n",
    "    The episode terminates at 200 time steps.\n",
    "    \n",
    "\n",
    "* **Arguments**\n",
    "\n",
    "    `g`: acceleration of gravity measured in *(m s<sup>-2</sup>)* used to calculate the pendulum dynamics. The default value is g = 10.0 .\n",
    "    ```\n",
    "    gym.make('Pendulum-v1', g=9.81)\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env2 = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "num_states = env2.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {} : x, y, omega \".format(num_states))\n",
    "num_actions = env2.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {} : torque\".format(num_actions))\n",
    "\n",
    "upper_bound = env2.action_space.high[0]\n",
    "lower_bound = env2.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))\n",
    "\n",
    "env2.reset()\n",
    "env2.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "play(env2, is_discrete=False, reward_to_win=-0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.1 Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "Just like the Actor-Critic method, we have two networks:\n",
    "\n",
    "* **Actor**: It proposes an action given a state.\n",
    "\n",
    "* **Critic**: It predicts if the action is good (positive value) or bad (negative value) given a state and an action.\n",
    "\n",
    "DDPG uses two more techniques not present in the original DQN:\n",
    "\n",
    "**First, it uses two Target networks.**\n",
    "\n",
    "Why? Because it add stability to training. In short, we are learning from estimated targets and Target networks are updated slowly, hence keeping our estimated targets stable.\n",
    "\n",
    "Conceptually, this is like saying, \"I have an idea of how to play this well, I'm going to try it out for a bit until I find something better\", as opposed to saying \"I'm going to re-learn how to play this entire game after every move\". Check [this StackOverflow answer](https://stackoverflow.com/a/54238556/13475679).\n",
    "\n",
    "**Second, it uses Experience Replay.**\n",
    "\n",
    "We store list of tuples (state, action, reward, next_state), and instead of learning only from recent experience, we learn from sampling all of our experience accumulated so far.\n",
    "\n",
    "Now, let's see how is it implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 Useful classes\n",
    "\n",
    "#### 2.2.1 The OUActionNoise class\n",
    "\n",
    "To implement better exploration by the Actor network, we use noisy perturbations, specifically an Ornstein-Uhlenbeck process for generating noise, as described in the paper. It samples noise from a correlated normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.2.2 The Buffer class\n",
    "\n",
    "The Buffer class implements Experience Replay.\n",
    "\n",
    "![https://i.imgur.com/mS6iGyJ.jpg](https://i.imgur.com/mS6iGyJ.jpg)\n",
    "\n",
    "* **Critic loss**: Mean Squared Error of y - Q(s, a) where y is the expected return as seen by the Target network, and Q(s, a) is action value predicted by the Critic network. y is a moving target that the critic model tries to achieve; we make this target stable by updating the Target model slowly.\n",
    "\n",
    "* **Actor loss**: This is computed using the mean of the value given by the Critic network for the actions taken by the Actor network. We seek to maximize this quantity.\n",
    "\n",
    "Hence we update the Actor network so that it produces actions that get the maximum predicted value as seen by the Critic, for a given state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') observation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
    "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
    "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
    "    @tf.function\n",
    "    def update(\n",
    "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
    "    ):\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + gamma * target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch, training=True)\n",
    "            critic_value = critic_model([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.3 NNs definition\n",
    "\n",
    "Here we define the Actor and Critic networks. These are basic Dense models with ReLU activation.\n",
    "\n",
    "Note: We need the initialization for last layer of the Actor to be between -0.003 and 0.003 as this prevents us from getting 1 or -1 output values in the initial stages, which would squash our gradients to zero, as we use the tanh activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_actor():\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Our upper bound is 2.0 for Pendulum.\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_critic():\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    # Both are passed through separate layer before concatenating\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's initialize a couple of actor and critic pairs. One pair will be the stable ones and the other will be used to explore possibilities.\n",
    "\n",
    "We also initialize the OUActionNoise and the Buffer classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Exploring pair\n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "# Stable pair\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "\n",
    "# Making the weights equal initially\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.002\n",
    "actor_lr = 0.001\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "std_dev = 0.2\n",
    "ou_noise = OUActionNoise(mean=np.zeros(1),  # this trigger the __init__ function\n",
    "                         std_deviation=float(std_dev) * np.ones(1))\n",
    "\n",
    "buffer = Buffer(50000, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.4 Policy definition and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def policy(state, noise_object):\n",
    "    \"\"\"  returns an action sampled from our Actor network \n",
    "         plus some noise for exploration. \"\"\"\n",
    "    sampled_actions = tf.squeeze(actor_model(state))  # to take just the value\n",
    "    noise = noise_object()    # this trigger the __call__ function\n",
    "    # Adding noise to action\n",
    "    sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return [np.squeeze(legal_action)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we implement our main training loop, and iterate over episodes. We sample actions using ``policy()`` and train with ``learn()`` at each time step, along with updating the Target networks at a rate ``tau``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Used to update target networks\n",
    "tau = 0.005\n",
    "total_episodes = 50\n",
    "\n",
    "# To store reward history of each episode\n",
    "ep_reward_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "\n",
    "# Takes about 4 min to train\n",
    "for ep in range(total_episodes):\n",
    "\n",
    "    prev_state = env2.reset()\n",
    "    episodic_reward = 0\n",
    "\n",
    "    while True:\n",
    "        if ep % 5 == 0:\n",
    "            env2.render()\n",
    "\n",
    "        tf_prev_state = np.expand_dims(prev_state, 0)\n",
    "\n",
    "        action = policy(tf_prev_state, ou_noise)\n",
    "        # Receive state and reward from environment.\n",
    "        state, reward, done, info = env2.step(action)\n",
    "\n",
    "        buffer.record((prev_state, action, reward, state))\n",
    "        episodic_reward += reward\n",
    "\n",
    "        buffer.learn()\n",
    "        update_target(target_actor.variables, actor_model.variables, tau)\n",
    "        update_target(target_critic.variables, critic_model.variables, tau)\n",
    "\n",
    "        # End this episode when `done` is True\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        prev_state = state\n",
    "\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "\n",
    "    # Mean of last 40 episodes\n",
    "    avg_reward = np.mean(ep_reward_list[-40:])\n",
    "    print(\"Episode * {} * final_mark is ==> {}\".format(ep, avg_reward))\n",
    "    avg_reward_list.append(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.plot(ep_reward_list)\n",
    "plt.legend([\"Averaged last 40\", \"Single episode\"])\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.5 Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "play(env2, target_actor, False, -0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The Inverted Pendulum problem has low complexity, but DDPG work great on many other problems.\n",
    "\n",
    "Another great environment to try this on is LunarLandingContinuous-v2, but it will take more episodes to obtain good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "217.458px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
